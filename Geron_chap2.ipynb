import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np


housing = pd.read_csv(r"C:\Users\KAsab\Desktop\PYTHON\california_housing.csv")


housing.head()


from sklearn.preprocessing import StandardScaler,MinMaxScaler,OneHotEncoder


housing.info()


housing["population"].hist(bins = 100)
plt.show()


np.log(housing["population"]).hist(edgecolor = "black",bins = 20)


housing["housing_median_age"].hist(edgecolor = "black",bins = 50)


from sklearn.metrics.pairwise import rbf_kernel


age_similarity = rbf_kernel(housing[["housing_median_age"]],[[35]],gamma = 0.1)


age_similarity


age_similarity = age_similarity.ravel()


plt.hist(age_similarity,bins = 10,color = "skyblue",edgecolor = "black")
plt.title("Histogram of RBF Similarity")
plt.xlabel("RBF Similarity")
plt.ylabel("Frequency")
plt.grid()
plt.show()


sns.kdeplot(age_similarity,fill = True,color = "blue")
plt.title("KDE Plot of RBF Similarity")
plt.xlabel("RBF Similarity")
plt.ylabel("Density")
plt.show()


sns.histplot(age_similarity, bins=10, kde=True, color='skyblue', edgecolor='black')
plt.title("Histogram with KDE Overlay")
plt.xlabel("RBF Similarity")
plt.ylabel("Frequency / Density")
plt.show()


housing["population"].hist(bins = 50)


from sklearn.preprocessing import FunctionTransformer





from sklearn.base import BaseEstimator,TransformerMixin


class CustomScaler(BaseEstimator,TransformerMixin):
    def __init__(self,feature_range = (0,1)):
        self.feature_range = feature_range
    def fit(self,X, y = None):
        self.min_ = X.min(axis = 0)
        self.max_ = X.max(axis = 0)
        return self
    def transform(self,X,y = None):
        X_scaled = (X - self.min_)/(self.max_ - self.min_)
        range_min,range_max = self.feature_range
        X_scaled = X_scaled * (range_max - range_min) + range_min
        return X_scaled
       


from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline,make_pipeline
from sklearn.impute import SimpleImputer


num_pipeline = Pipeline([
    ("impute",SimpleImputer(strategy = "median")),
    ("standardize",StandardScaler()),
])


num_pipeline


num_pipeline2 = make_pipeline(SimpleImputer(strategy = "median"),StandardScaler())


num_pipeline2


housing_num = housing.select_dtypes(include = "number")


housing_num.head()


housing_num_prepared = num_pipeline.fit_transform(housing_num)





housing_num_prepared[:2].round(2)





df_housing_num_prepared = pd.DataFrame(
    housing_num_prepared,columns = num_pipeline.get_feature_names_out(),
    index = housing_num.index
)


df_housing_num_prepared.head()


from sklearn.compose import ColumnTransformer


num_attribs = housing.select_dtypes(include = "number").columns.to_list()


num_attribs


cat_attribs = housing.select_dtypes(include = "object").columns.to_list()


cat_attribs


cat_pipeline = make_pipeline(SimpleImputer(strategy = "most_frequent"),
                             OneHotEncoder(handle_unknown = "ignore",sparse_output = False))





preprocessing = ColumnTransformer([
    ("num",num_pipeline,num_attribs),
    ("cat",cat_pipeline,cat_attribs)
])








from sklearn.compose import make_column_selector,make_column_transformer


preprocessing2 = make_column_transformer(
    (num_pipeline,make_column_selector(dtype_include = np.number)),
    (cat_pipeline,make_column_selector(dtype_include = object))
)


housing_prepared = preprocessing.fit_transform(housing)


housing_prepared[:3].round(2)


housing_prepared_df = pd.DataFrame(housing_prepared,columns = preprocessing.get_feature_names_out(),index = housing.index)


housing_prepared_df


class ClusterSimilarity(BaseEstimator, TransformerMixin):
    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
        self.n_clusters = n_clusters
        self.gamma = gamma
        self.random_state = random_state
    def fit(self, X, y=None, sample_weight=None):
        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)
        self.kmeans_.fit(X, sample_weight=sample_weight)
        return self # always return self!
    def transform(self, X):
        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)
    def get_feature_names_out(self, names=None):
        return [f"Cluster {i} similarity" for i in range(self.n_clusters)]


def column_ratio(X):
    return X[:, [0]] / X[:, [1]]
def ratio_name(function_transformer, feature_names_in):
    return ["ratio"] # feature names out
def ratio_pipeline():
    return make_pipeline(
SimpleImputer(strategy="median"),
FunctionTransformer(column_ratio, feature_names_out=ratio_name),
StandardScaler())
log_pipeline = make_pipeline(
SimpleImputer(strategy="median"),
FunctionTransformer(np.log, feature_names_out="one-to-one"),
StandardScaler())
cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)
default_num_pipeline = make_pipeline(SimpleImputer(strategy="median"),
StandardScaler())
preprocessing = ColumnTransformer([
("bedrooms", ratio_pipeline(), ["total_bedrooms", "total_rooms"]),
("rooms_per_house", ratio_pipeline(), ["total_rooms", "households"]),
("people_per_house", ratio_pipeline(), ["population", "households"]),
("log", log_pipeline, ["total_bedrooms", "total_rooms", "population",
"households", "median_income"]),
("geo", cluster_simil, ["latitude", "longitude"]),
("cat", cat_pipeline, make_column_selector(dtype_include=object)),
],
remainder=default_num_pipeline) # one column remaining:housing_median_age


housing_prepared = preprocessing.fit_transform(housing)


housing_prepared.shape


preprocessing.get_feature_names_out()





housing.head()


## X = housing.iloc[:, housing.columns != housing.columns[8]]
X = housing.drop(columns = ["median_house_value"])
## y  = housing.iloc[:, 8]
y = housing["median_house_value"]


X.head()


y.head()


from sklearn.model_selection import train_test_split


X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)


housing_features = X.copy()
housing_labels = y.copy()


X_train.shape





from sklearn.linear_model import LinearRegression


lin_reg = make_pipeline(preprocessing,LinearRegression())


lin_model = lin_reg.fit(housing_features,housing_labels)


lin_model


from sklearn.metrics import root_mean_squared_error


housing_predictions = lin_model.predict(housing_features)


lin_rmse = root_mean_squared_error(housing_labels,housing_predictions)


lin_rmse





from sklearn.tree import DecisionTreeRegressor


tree_reg = make_pipeline(preprocessing,DecisionTreeRegressor(random_state = 42))


tree_reg.fit(housing_features,housing_labels)


housing_predictions_tree = tree_reg.predict(housing_features)


rmse = root_mean_squared_error


rmse(housing_labels,housing_predictions_tree)





from sklearn.model_selection import cross_val_score


trees_rmses = -cross_val_score(tree_reg,housing_features,housing_labels,scoring = "neg_root_mean_squared_error",cv = 10)


trees_rmses








from sklearn.ensemble import RandomForestRegressor


forest_reg = make_pipeline(preprocessing, RandomForestRegressor(random_state = 42))
forest_rmses = -cross_val_score(forest_reg,housing_features,housing_labels,scoring = "neg_root_mean_squared_error",cv = 10)


pd.Series(forest_rmses).describe()





from sklearn.model_selection import GridSearchCV


full_pipeline = Pipeline([
    ("preprocessing",preprocessing),
    ("random_forest",RandomForestRegressor(random_state = 42)),
])


param_grid = [
{'preprocessing__geo__n_clusters': [5, 8, 10],
'random_forest__max_features': [4, 6, 8]},
{'preprocessing__geo__n_clusters': [10, 15],
'random_forest__max_features': [6, 8, 10]},
]


grid_search = GridSearchCV(full_pipeline,param_grid, cv = 3, scoring = "neg_root_mean_squared_error")


grid_search.fit(housing_features,housing_labels)


grid_search.best_params_















